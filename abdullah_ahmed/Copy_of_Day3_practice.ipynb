{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Text Corpora & Preprocessing"
      ],
      "metadata": {
        "id": "9YC48QxqFYBq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcw5nuxKFMdG",
        "outputId": "5cb4e0f4-d95d-418b-f24e-2cf8b0670072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Load and view a sample\n",
        "sample = gutenberg.raw('austen-emma.txt')[:500]\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task1:\n",
        "#Print the first 300 characters from a different book\n",
        "from nltk.corpus import gutenberg\n",
        "sample = gutenberg.raw('shakespeare-macbeth.txt')[:300]\n",
        "print(sample)\n",
        "print ('---------------------------------------------------------------------')\n",
        "#Try basic cleanup: lowercase, remove punctuation\n",
        "import re\n",
        "cleaned = sample.lower()\n",
        "print(re.sub(r'[^\\s\\w]','',cleaned))"
      ],
      "metadata": {
        "id": "91HRVpyzFfMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d390aa-14fd-4332-d074-73199dc16e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[The Tragedie of Macbeth by William Shakespeare 1603]\n",
            "\n",
            "\n",
            "Actus Primus. Scoena Prima.\n",
            "\n",
            "Thunder and Lightning. Enter three Witches.\n",
            "\n",
            "  1. When shall we three meet againe?\n",
            "In Thunder, Lightning, or in Raine?\n",
            "  2. When the Hurley-burley's done,\n",
            "When the Battaile's lost, and wonne\n",
            "\n",
            "   3. That will be ere \n",
            "---------------------------------------------------------------------\n",
            "the tragedie of macbeth by william shakespeare 1603\n",
            "\n",
            "\n",
            "actus primus scoena prima\n",
            "\n",
            "thunder and lightning enter three witches\n",
            "\n",
            "  1 when shall we three meet againe\n",
            "in thunder lightning or in raine\n",
            "  2 when the hurleyburleys done\n",
            "when the battailes lost and wonne\n",
            "\n",
            "   3 that will be ere \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bag of Words (BoW)"
      ],
      "metadata": {
        "id": "fRK6e_cmFt2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "docs = [\"I love NLP\", \"NLP is fun and powerful\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(docs)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8hQVPQDFzhO",
        "outputId": "42386735-5612-470d-9cd4-1c32d5206e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and' 'fun' 'is' 'love' 'nlp' 'powerful']\n",
            "[[0 0 0 1 1 0]\n",
            " [1 1 1 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task:\n",
        "# Add a third document\n",
        "docs = [\"I love NLP\", \"NLP is fun and powerful\",\"NLP is part of AI\"]\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Check how BoW vector changes\n",
        "\n",
        "X = vectorizer.fit_transform(docs)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "id": "JuHcfza3F6zL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb4c345-fa4f-4036-fe7d-8ec4034cff4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ai' 'and' 'fun' 'is' 'love' 'nlp' 'of' 'part' 'powerful']\n",
            "[[0 0 0 0 1 1 0 0 0]\n",
            " [0 1 1 1 0 1 0 0 1]\n",
            " [1 0 0 1 0 1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TF-IDF"
      ],
      "metadata": {
        "id": "h2VcayyHGRsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X = tfidf.fit_transform(docs)\n",
        "\n",
        "print(tfidf.get_feature_names_out())\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXRJs8P4GXCw",
        "outputId": "7fecfd4f-383d-4958-898c-6a6a62f58db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ai' 'and' 'fun' 'is' 'love' 'nlp' 'of' 'part' 'powerful']\n",
            "[[0.         0.         0.         0.         0.861037   0.50854232\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.50461134 0.50461134 0.38376993 0.         0.29803159\n",
            "  0.         0.         0.50461134]\n",
            " [0.50461134 0.         0.         0.38376993 0.         0.29803159\n",
            "  0.50461134 0.50461134 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task:\n",
        "# Compare BoW and TF-IDF outputs side by side\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "docs = [\"I love NLP\", \"NLP is fun and powerful\",\"NLP is part of AI\"]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(docs)\n",
        "\n",
        "print(\"Bag of Words:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X_bow.toarray())\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(docs)\n",
        "\n",
        "print(\"\\nTF-IDF:\")\n",
        "print(tfidf.get_feature_names_out())\n",
        "print(X_tfidf.toarray())"
      ],
      "metadata": {
        "id": "MIx9RpTvGZlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e4efba-9346-4ae2-b3cd-edc267a9ad61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words:\n",
            "['ai' 'and' 'fun' 'is' 'love' 'nlp' 'of' 'part' 'powerful']\n",
            "[[0 0 0 0 1 1 0 0 0]\n",
            " [0 1 1 1 0 1 0 0 1]\n",
            " [1 0 0 1 0 1 1 1 0]]\n",
            "\n",
            "TF-IDF:\n",
            "['ai' 'and' 'fun' 'is' 'love' 'nlp' 'of' 'part' 'powerful']\n",
            "[[0.         0.         0.         0.         0.861037   0.50854232\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.50461134 0.50461134 0.38376993 0.         0.29803159\n",
            "  0.         0.         0.50461134]\n",
            " [0.50461134 0.         0.         0.38376993 0.         0.29803159\n",
            "  0.50461134 0.50461134 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Embeddings (Pre-trained)"
      ],
      "metadata": {
        "id": "xqT6rUklGgqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dPzHAErGp_x",
        "outputId": "0850dc15-5447-4b93-801f-2743109d576f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "print(model['king'])  # embedding of 'king'\n",
        "print(model.similarity('king', 'queen'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqFLT0dFGmZB",
        "outputId": "79e98421-174e-4b81-8112-2ad8f98a3f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "[-0.32307  -0.87616   0.21977   0.25268   0.22976   0.7388   -0.37954\n",
            " -0.35307  -0.84369  -1.1113   -0.30266   0.33178  -0.25113   0.30448\n",
            " -0.077491 -0.89815   0.092496 -1.1407   -0.58324   0.66869  -0.23122\n",
            " -0.95855   0.28262  -0.078848  0.75315   0.26584   0.3422   -0.33949\n",
            "  0.95608   0.065641  0.45747   0.39835   0.57965   0.39267  -0.21851\n",
            "  0.58795  -0.55999   0.63368  -0.043983 -0.68731  -0.37841   0.38026\n",
            "  0.61641  -0.88269  -0.12346  -0.37928  -0.38318   0.23868   0.6685\n",
            " -0.43321  -0.11065   0.081723  1.1569    0.78958  -0.21223  -2.3211\n",
            " -0.67806   0.44561   0.65707   0.1045    0.46217   0.19912   0.25802\n",
            "  0.057194  0.53443  -0.43133  -0.34311   0.59789  -0.58417   0.068995\n",
            "  0.23944  -0.85181   0.30379  -0.34177  -0.25746  -0.031101 -0.16285\n",
            "  0.45169  -0.91627   0.64521   0.73281  -0.22752   0.30226   0.044801\n",
            " -0.83741   0.55006  -0.52506  -1.7357    0.4751   -0.70487   0.056939\n",
            " -0.7132    0.089623  0.41394  -1.3363   -0.61915  -0.33089  -0.52881\n",
            "  0.16483  -0.98878 ]\n",
            "0.7507691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task:\n",
        "# Try similarity between different word pairs\n",
        "#print(model['king'])  # embedding of 'king'\n",
        "print(model.similarity('man', 'women'))\n",
        "print(model.similarity('doctor', 'hospital'))\n",
        "print(model.similarity('cat', 'dog'))"
      ],
      "metadata": {
        "id": "TM9Z8OyvH2YG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af94be75-6318-4de3-e6fd-5c03bc95f24a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5303662\n",
            "0.69009304\n",
            "0.8798075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cosine Similarity Between TextsA"
      ],
      "metadata": {
        "id": "MOLK0s1xGy5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity = cosine_similarity(X[0], X[1])\n",
        "print(\"Cosine similarity between doc 1 and doc 2:\", similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn-CEWTQGo8E",
        "outputId": "69ca903b-cd98-49a5-b644-c7408edfbf41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between doc 1 and doc 2: [[0.15156167]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# task:\n",
        "# Try with 3+ documents, find the most similar pair\n",
        "docs = [\"I love NLP\", \"NLP is fun and powerful\",\"NLP is part of AI\"]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
        "\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "print(similarity_matrix)"
      ],
      "metadata": {
        "id": "dC5ALp0YG6EU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7beb1846-db8c-4d1b-d834-3f113bd1b09a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.15156167 0.15156167]\n",
            " [0.15156167 1.         0.23610219]\n",
            " [0.15156167 0.23610219 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Precision, Recall, F1-Score & Confusion Matrix"
      ],
      "metadata": {
        "id": "6LORSMe3HFU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Example data\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0]\n",
        "y_pred = [1, 0, 1, 0, 0, 1, 1]\n",
        "\n",
        "print(\"Precision:\", precision_score(y_true, y_pred))\n",
        "print(\"Recall:\", recall_score(y_true, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5imc9fAHI2q",
        "outputId": "b0599d1d-b70f-4928-f826-4dac76bd37e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.75\n",
            "Recall: 0.75\n",
            "F1 Score: 0.75\n",
            "Confusion Matrix:\n",
            " [[2 1]\n",
            " [1 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# task:\n",
        "# Change predictions and observe metric changes\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Example data\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0]\n",
        "y_pred = [1, 0, 1, 1, 0, 0, 0]\n",
        "\n",
        "print(\"Precision:\", precision_score(y_true, y_pred))\n",
        "print(\"Recall:\", recall_score(y_true, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "f0Tlx2VgHLI3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf45185-2f57-44c1-e5d6-52ec885b3266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 0.75\n",
            "F1 Score: 0.8571428571428571\n",
            "Confusion Matrix:\n",
            " [[3 0]\n",
            " [1 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Co-occurrence (Intro)"
      ],
      "metadata": {
        "id": "l4rezK1RHQKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEqKlQpjIcyd",
        "outputId": "0d2d4732-456b-465e-ee30-8f9de9f83714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "nltk.download('reuters')\n",
        "\n",
        "# Use Treebank tokenizer (no 'punkt' needed)\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "# Load and tokenize text\n",
        "text = reuters.raw(fileids=['test/14826'])[:1000]\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "# Build co-occurrence window\n",
        "window_size = 2\n",
        "pairs = []\n",
        "\n",
        "for i in range(len(tokens) - window_size):\n",
        "    window = tokens[i:i + window_size + 1]\n",
        "    for j in range(1, len(window)):\n",
        "        pairs.append((window[0], window[j]))\n",
        "\n",
        "# Count and print top word pairs\n",
        "co_occurrence = Counter(pairs)\n",
        "print(co_occurrence.most_common(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoGb-lYsIwpH",
        "outputId": "382e6167-1976-4338-f1c5-ee6cb8fc1344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('in', 'the'), 3), (('the', 'U.S.'), 2), (('the', 'And'), 2), (('U.S.', 'And'), 2), (('that', 'the'), 2), (('on', 'imports'), 2), (('imports', 'of'), 2), ((',', 'in'), 2), (('of', 'tariffs'), 2), (('ASIAN', 'EXPORTERS'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task:\n",
        "# Try changing window_size, and observe the new top pairs\n",
        "window_size = 3\n",
        "pairs = []\n",
        "\n",
        "for i in range(len(tokens) - window_size):\n",
        "    window = tokens[i:i + window_size + 1]\n",
        "    for j in range(1, len(window)):\n",
        "        pairs.append((window[0], window[j]))\n",
        "\n",
        "# Count and print top word pairs\n",
        "co_occurrence = Counter(pairs)\n",
        "print(co_occurrence.most_common(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6cgfpe6HWxk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef691dd-23bd-4876-e810-0d6853f34e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('in', 'the'), 3), (('the', 'U.S.'), 2), (('the', 'And'), 2), (('U.S.', 'And'), 2), (('U.S.', 'Japan'), 2), (('that', 'the'), 2), (('to', 'on'), 2), (('on', 'imports'), 2), (('on', 'of'), 2), (('imports', 'of'), 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CM9jlYVSeC42"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}