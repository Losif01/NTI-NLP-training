# -*- coding: utf-8 -*-
"""Day_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mKa3Can0dN0SQrAvc8EKV2wstwCA5K2y

# Code: Tokenization(Sentences)
"""

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.tokenize import sent_tokenize, word_tokenize

example_string = """
Muad'Dib learned rapidly because his first training was in how to learn.
And the first lesson of all was the basic trust that he could learn.
It's shocking to find how many people do not believe they can learn,
and how many more believe learning to be difficult."""

sent_tokenize(example_string)

"""# Code: Tokenization(Words)"""

import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize, word_tokenize

example_string = """
Muad'Dib learned rapidly because his first training was in how to learn.
And the first lesson of all was the basic trust that he could learn.
It's shocking to find how many people do not believe they can learn,
and how many more believe learning to be difficult."""

word_tokenize(example_string)

"""# Code: Tokenization(N-Grams)"""

import nltk
nltk.download('punkt')
from nltk.util import ngrams

example_string = """
Muad'Dib learned rapidly because his first training was in how to learn.
And the first lesson of all was the basic trust that he could learn.
It's shocking to find how many people do not believe they can learn,
and how many more believe learning to be difficult."""

tokens = word_tokenize(example_string)
bigrams = list(ngrams(tokens, 2))

# Print bigrams
for bg in bigrams:
    print(bg)

"""# Tokenization: Regular Expressions"""

from nltk.tokenize import RegexpTokenizer

my_text = "Hi Mr. Smith! I'm going to buy some vegetables (tomatoes"
" and cucumbers) from the store. Should I pick up some black-eyed " \
"peas as well?"

whitespace_tokenizer = RegexpTokenizer("\s+", gaps=True)
print(whitespace_tokenizer.tokenize(my_text))

from nltk.tokenize import RegexpTokenizer

cap_tokenizer = RegexpTokenizer("[A-Z]['\w]+")
print(cap_tokenizer.tokenize(my_text))

"""# TokenizationSummary

With tokenization, we were able to break this messy text data down into small units for us
 to do analysis

*   By sentence, word, n-grams
*   By characters and patterns using regularexpression

# Zipf's Law and Word Frequency
"""

from collections import Counter
import matplotlib.pyplot as plt

freq = Counter(tokens)
most_common = freq.most_common(20)
words, counts = zip(*most_common)

plt.bar(words, counts)
plt.xticks(rotation=45)
plt.title("Top 20 Word Frequencies")
plt.show()

import numpy as np

ranks = np.arange(1, len(freq)+1)
frequencies = np.array(sorted(freq.values(), reverse=True))

plt.loglog(ranks, frequencies)
plt.xlabel("Rank")
plt.ylabel("Frequency")
plt.title("Zipf's Law")
plt.grid(True)
plt.show()

"""# Maximum Matching Algorithm"""

wordlist = {"the", "cat", "in", "the", "hat"}
text = "thecatinthehat"

def max_match(text, wordlist):
    if not text:
        return []
    for i in range(len(text), 0, -1):
        first = text[:i]
        if first in wordlist:
            return [first] + max_match(text[i:], wordlist)
    return [text]

print(max_match(text, wordlist))

"""# Normalize Arabic"""

import re

text = "Mr. Smith bought 2lbs of tomatoes and 5 cucumbers."
text = re.sub(r'\d+', '', text)  # remove digits
text = re.sub(r'[^\w\s]', '', text)  # remove punctuation
text = text.lower()
print(text)

def normalize_arabic(text):
    text = re.sub("[إأآا]", "ا", text)
    text = re.sub("ى", "ي", text)
    text = re.sub("ؤ", "و", text)
    text = re.sub("ئ", "ي", text)
    return text

arabic = "إِنَّ الَّذِينَ آمَنُوا وَعَمِلُوا الصَّالِحَاتِ"
print(normalize_arabic(arabic))