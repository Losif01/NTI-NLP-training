# -*- coding: utf-8 -*-
"""D17 scrapping altibbi website.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d4ApbqOK_n_xIA6xtsVMpPk6wtZkCs2f
"""



"""## scrap using beautifulsoup4"""

!pip install requests beautifulsoup4 pandas tqdm

import requests
import pandas as pd
from bs4 import BeautifulSoup
from tqdm.notebook import tqdm
import time
import re

# --- Configuration ---
BASE_URL = "https://altibbi.com"
# This is the base page where the category selector exists
QUESTIONS_BASE_PATH = "/Ø§Ø³Ø¦Ù„Ø©-Ø·Ø¨ÙŠØ©"
TARGET_QUESTIONS_PER_CATEGORY = 200
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# --- Data Storage ---
# We'll use a dictionary to hold a separate DataFrame for each category
all_dataframes = {}


# --- Function to Scrape a Single Category ---
def scrape_category(category_name, category_slug, target_count):
    """
    Scrapes a specific category from Altibbi until a target number of Q&As is reached.

    Args:
        category_name (str): The human-readable name of the category (e.g., "Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù„Ø¯ÙŠØ©").
        category_slug (str): The URL part for the category (e.g., "Ø§Ù„Ø§Ù…Ø±Ø§Ø¶-Ø§Ù„Ø¬Ù„Ø¯ÙŠØ©").
        target_count (int): The number of Q&As to scrape.

    Returns:
        pandas.DataFrame: A DataFrame containing the 'question' and 'answer' columns.
    """
    print(f"\n{'='*20}\nStarting scrape for category: {category_name}\n{'='*20}")
    qa_data = []
    page_num = 1
    start_path = f"{QUESTIONS_BASE_PATH}/{category_slug}"

    with tqdm(total=target_count, desc=f"Scraping {category_name}") as pbar:
        while len(qa_data) < target_count:
            if page_num == 1:
                current_url = f"{BASE_URL}{start_path}"
            else:
                current_url = f"{BASE_URL}{start_path}?page={page_num}"

            try:
                response = requests.get(current_url, headers=HEADERS, timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
                question_articles = soup.find_all('article', class_='new-question-item')

                if not question_articles:
                    print(f"No more questions found for '{category_name}'. Stopping at {len(qa_data)} items.")
                    break

                for article in question_articles:
                    question_tag = article.find('h2', class_='question-text')
                    answer_container = article.find('div', itemprop='acceptedAnswer')
                    answer_tag = answer_container.find('div', itemprop='text') if answer_container else None

                    if question_tag and answer_tag:
                        qa_data.append({
                            'question': question_tag.get_text(strip=True),
                            'answer': answer_tag.get_text(strip=True)
                        })
                        pbar.update(1)

                    if len(qa_data) >= target_count:
                        break

                page_num += 1
                time.sleep(1)

            except requests.RequestException as e:
                print(f"An error occurred for '{category_name}' on page {page_num}: {e}")
                break

    return pd.DataFrame(qa_data)


# --- Step 1: Fetch and Display Categories ---
def get_and_select_categories():
    """
    Fetches the list of categories, displays them, and prompts the user for a selection.
    """
    print("Fetching available medical categories...")
    try:
        response = requests.get(BASE_URL + QUESTIONS_BASE_PATH, headers=HEADERS)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find the <select> element by its ID
        select_element = soup.find('select', id='w1')
        if not select_element:
            print("Could not find the category selector dropdown. Exiting.")
            return []

        # Extract options: text (name) and value (slug)
        options = select_element.find_all('option')
        category_options = []
        for option in options:
            # Ignore the first placeholder option which has no value
            if option.get('value'):
                category_options.append({
                    "name": option.get_text(strip=True),
                    "slug": option['value']
                })

        print("\nPlease choose the categories you want to scrape.")
        print("Enter the numbers separated by commas (e.g., 1, 5, 10)")
        for i, cat in enumerate(category_options):
            print(f"{i+1}: {cat['name']}")

        while True:
            user_input = input("\nEnter your choices: ")
            try:
                selected_indices = [int(i.strip()) - 1 for i in user_input.split(',')]
                # Validate input
                if all(0 <= i < len(category_options) for i in selected_indices):
                    selected_categories = [category_options[i] for i in selected_indices]
                    return selected_categories
                else:
                    print("Invalid input. Please enter numbers from the list above.")
            except ValueError:
                print("Invalid format. Please enter numbers separated by commas.")

    except requests.RequestException as e:
        print(f"Failed to fetch category list: {e}")
        return []

# --- Main Execution ---
selected_categories_to_scrape = get_and_select_categories()

if selected_categories_to_scrape:
    print(f"\nOK! Will scrape the following {len(selected_categories_to_scrape)} categories:")
    for cat in selected_categories_to_scrape:
        print(f"- {cat['name']}")

    for category in selected_categories_to_scrape:
        # Scrape the data and store the resulting DataFrame in our dictionary
        df = scrape_category(category['name'], category['slug'], TARGET_QUESTIONS_PER_CATEGORY)
        all_dataframes[category['name']] = df

    print("\n\nAll selected categories have been scraped!")
else:
    print("\nNo categories selected. The script will now exit.")

if not all_dataframes:
    print("No data was scraped. Nothing to save.")
else:
    print("\n--- Saving Results to CSV Files ---")
    for category_name, df in all_dataframes.items():
        print(f"\nPreview for '{category_name}':")
        display(df.head(5))
        print(f"DataFrame shape: {df.shape}")

        # Create a clean filename from the category name
        # Removes special characters and replaces spaces with underscores
        clean_name = re.sub(r'[^\w\s-]', '', category_name).strip().replace(' ', '_')
        file_name = f"altibbi_qa_{clean_name}.csv"

        # Save to CSV with UTF-8-SIG encoding for proper Arabic support in Excel
        df.to_csv(file_name, index=False, encoding='utf-8-sig')
        print(f"âœ… Successfully saved data to '{file_name}'")

    print("\nAll files saved. Check the file explorer on the left.")

"""### classify

## scrap using selenium
"""

# Cell 1: Environment Setup
# -------------------------
# This cell installs the required Python libraries (selenium) and the
# Chromium browser/driver needed for Selenium to run in the Colab environment.

print("ðŸ“¦ Installing required packages and browser driver...")
!pip install selenium -q
!apt-get update -q
!apt-get install -y chromium-chromedriver -q
print("âœ… Setup complete.")

# Cell 2: Imports and Global Configuration
# ----------------------------------------
# Here we import all necessary libraries and define the global variables
# that will be used throughout the script.

import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from tqdm.notebook import tqdm
import re

# --- Main Configuration ---
BASE_URL = "https://altibbi.com"
QUESTIONS_BASE_PATH = "/Ø§Ø³Ø¦Ù„Ø©-Ø·Ø¨ÙŠØ©"

# --- Data Storage ---
# This dictionary will hold the final DataFrames for each scraped category.
all_dataframes = {}

# Cell 3: Core Function Definitions
# ---------------------------------
# This cell defines the primary functions for our scraping task.

def setup_driver():
    """Configures and returns a headless Selenium WebDriver for Colab."""
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    return webdriver.Chrome(options=chrome_options)

def get_and_select_categories(driver):
    """Uses Selenium to find all categories and prompts the user for a selection."""
    print("ðŸ”Ž Fetching available medical categories...")
    try:
        driver.get(BASE_URL + QUESTIONS_BASE_PATH)
        wait = WebDriverWait(driver, 20)
        select_element = wait.until(EC.presence_of_element_located((By.ID, 'w1')))
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        options = soup.select('#w1 option')
        category_options = [
            {"name": o.get_text(strip=True), "slug": o['value']} for o in options if o.get('value')
        ]

        print("\nPlease choose the categories you want to scrape.")
        print("Enter the numbers separated by commas (e.g., 2, 9, 15)")
        for i, cat in enumerate(category_options):
            print(f"{i+1}: {cat['name']}")

        while True:
            user_input = input("\nEnter your choices: ")
            try:
                selected_indices = [int(i.strip()) - 1 for i in user_input.split(',')]
                if all(0 <= i < len(category_options) for i in selected_indices):
                    return [category_options[i] for i in selected_indices]
                else: print("Invalid input. Please enter numbers from the list.")
            except ValueError: print("Invalid format. Please enter numbers separated by commas.")
    except Exception as e:
        print(f"An error occurred while fetching categories: {e}")
        return []

def get_scrape_target():
    """Asks the user if they want to scrape all items or a specific number."""
    print("\nHow many records do you want to scrape per category?")
    print("  1: Scrape a specific number of questions.")
    print("  2: Scrape ALL available questions (this might take a long time).")
    while True:
        choice = input("Enter your choice (1 or 2): ").strip()
        if choice == '1':
            while True:
                try:
                    num = int(input("Enter the number of questions to scrape: ").strip())
                    if num > 0: return num
                    else: print("Please enter a positive number.")
                except ValueError: print("Invalid input. Please enter a number.")
        elif choice == '2':
            print("OK, will scrape all available questions.")
            # We use float('inf') to represent an unlimited target
            return float('inf')
        else:
            print("Invalid choice. Please enter 1 or 2.")

def scrape_category_selenium(driver, category_name, category_slug, target_count):
    """Scrapes a specific category. Handles both a fixed target and an 'all' target."""
    print(f"\n{'='*20}\nStarting scrape for category: {category_name}\n{'='*20}")
    qa_data = []
    page_num = 1

    # If target is infinite, tqdm shows an indeterminate progress bar.
    tqdm_total = target_count if target_count != float('inf') else None

    with tqdm(total=tqdm_total, desc=f"Scraping {category_name}") as pbar:
        while len(qa_data) < target_count:
            current_url = f"{BASE_URL}{QUESTIONS_BASE_PATH}/{category_slug}" + (f"?page={page_num}" if page_num > 1 else "")
            try:
                driver.get(current_url)
                WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "article.new-question-item")))
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                question_articles = soup.find_all('article', class_='new-question-item')

                if not question_articles:
                    pbar.close()
                    print(f"No more questions found. Stopping scrape for '{category_name}'.")
                    break

                for article in question_articles:
                    question_tag = article.find('h2', class_='question-text')
                    answer_container = article.find('div', itemprop='acceptedAnswer')
                    answer_tag = answer_container.find('div', itemprop='text') if answer_container else None
                    if question_tag and answer_tag:
                        qa_data.append({'question': question_tag.get_text(strip=True), 'answer': answer_tag.get_text(strip=True)})
                        pbar.update(1)
                    if len(qa_data) >= target_count:
                        break
                page_num += 1
            except Exception as e:
                pbar.close()
                print(f"\nAn error occurred on page {page_num} for '{category_name}': {e}")
                break
    return pd.DataFrame(qa_data)

print("âœ… Core functions defined successfully.")

# Cell 4: Initialize Driver and Get User Selections
# -------------------------------------------------
# This is the interactive part of the script. It starts the browser,
# then asks you to choose the categories and how many records to scrape.

driver = setup_driver()

# Step 1: Get category selections
selected_categories_to_scrape = get_and_select_categories(driver)

# Step 2: Get the target number of records, only if categories were selected
scrape_target_count = None
if selected_categories_to_scrape:
    scrape_target_count = get_scrape_target()

# Cell 5: Execute the Scraping Process
# ------------------------------------
# This cell runs the main scraping loop based on your selections from the
# previous cell. After it's done, it will safely close the browser.

try:
    if selected_categories_to_scrape and scrape_target_count is not None:
        print(f"\nðŸš€ Starting the scraping process for {len(selected_categories_to_scrape)} categories...")
        for category in selected_categories_to_scrape:
            df = scrape_category_selenium(
                driver,
                category['name'],
                category['slug'],
                scrape_target_count  # Use the user-defined target
            )
            all_dataframes[category['name']] = df
        print("\n\nðŸŽ‰ All selected categories have been scraped!")
    else:
        print("\nNo categories were selected, or the process was cancelled. Nothing to scrape.")

finally:
    print("\nClosing the Selenium WebDriver...")
    driver.quit()
    print("âœ… WebDriver closed.")

# Cell 6: Review and Save Results
# -------------------------------
# This final cell will display the first few rows of each DataFrame that was
# created and then save each one to its own CSV file.

if not all_dataframes:
    print("\nNo data was scraped, nothing to save.")
else:
    print("\n--- Saving Results to CSV Files ---")
    for category_name, df in all_dataframes.items():
        if df.empty:
            print(f"\nDataFrame for '{category_name}' is empty. Skipping file save.")
            continue

        print(f"\nðŸ“Š Preview for '{category_name}':")
        display(df.head(5))
        print(f"   Total rows collected: {df.shape[0]}")

        # Create a clean, safe filename from the category name
        clean_name = re.sub(r'[^\w\s-]', '', category_name).strip().replace(' ', '_')
        file_name = f"altibbi_qa_{clean_name}.csv"

        # Save to CSV with UTF-8-SIG encoding for proper Arabic support
        df.to_csv(file_name, index=False, encoding='utf-8-sig')
        print(f"   ðŸ’¾ Successfully saved data to '{file_name}'")

    print("\n\nâœ… All files saved. Check the file explorer on the left.")