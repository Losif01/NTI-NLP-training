# -*- coding: utf-8 -*-
"""Day3 practice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MSnvVnjGdIPcwKqaCkFkxK6n13nokfvk

#Text Corpora & Preprocessing
"""

import nltk
nltk.download('gutenberg')
from nltk.corpus import gutenberg

# Load and view a sample
sample = gutenberg.raw('austen-emma.txt')[:500]
print(sample)

#Task1:
#Print the first 300 characters from a different book


#Try basic cleanup: lowercase, remove punctuation

"""#Bag of Words (BoW)"""

from sklearn.feature_extraction.text import CountVectorizer

docs = ["I love NLP", "NLP is fun and powerful"]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(docs)

print(vectorizer.get_feature_names_out())
print(X.toarray())

# Task:
# Add a third document

# Check how BoW vector changes

"""#TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
X = tfidf.fit_transform(docs)

print(tfidf.get_feature_names_out())
print(X.toarray())

#  Task:
# Compare BoW and TF-IDF outputs side by side

"""#Word Embeddings (Pre-trained)"""

#pip install gensim

import gensim.downloader as api
model = api.load("glove-wiki-gigaword-100")

print(model['king'])  # embedding of 'king'
print(model.similarity('king', 'queen'))

# Task:
# Try similarity between different word pairs

"""#Cosine Similarity Between TextsA"""

from sklearn.metrics.pairwise import cosine_similarity

similarity = cosine_similarity(X[0], X[1])
print("Cosine similarity between doc 1 and doc 2:", similarity)

# task:
# Try with 3+ documents, find the most similar pair

"""#Precision, Recall, F1-Score & Confusion Matrix"""

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

# Example data
y_true = [1, 0, 1, 1, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 1]

print("Precision:", precision_score(y_true, y_pred))
print("Recall:", recall_score(y_true, y_pred))
print("F1 Score:", f1_score(y_true, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))

# task:
# Change predictions and observe metric changes

"""#Word Co-occurrence (Intro)"""

import nltk
nltk.download('punkt')

from collections import Counter
import nltk
from nltk.corpus import reuters
from nltk.tokenize import TreebankWordTokenizer

nltk.download('reuters')

# Use Treebank tokenizer (no 'punkt' needed)
tokenizer = TreebankWordTokenizer()

# Load and tokenize text
text = reuters.raw(fileids=['test/14826'])[:1000]
tokens = tokenizer.tokenize(text)

# Build co-occurrence window
window_size = 2
pairs = []

for i in range(len(tokens) - window_size):
    window = tokens[i:i + window_size + 1]
    for j in range(1, len(window)):
        pairs.append((window[0], window[j]))

# Count and print top word pairs
co_occurrence = Counter(pairs)
print(co_occurrence.most_common(10))

# Task:
# Try changing window_size, and observe the new top pairs